# CSV Catalog and Lineage

Last updated: 2026-02-22

This document describes all CSV/CSV.GZ files currently in `analysis/`, including:
- what each file is for,
- how it was created (or where it came from),
- what its entries/columns represent.

## 1) Inventory (all CSVs in repo)

| Path | Rows | Cols | Role |
|---|---:|---:|---|
| `analysis/data/raw/digital/2024/google/google2024_set1_20250715.csv.gz` | 102,049 | 92 | Raw input (Google 2024 digital ads) |
| `analysis/data/raw/digital/2024/meta/meta2024_set1_20250714.csv.gz` | 477,024 | 109 | Raw input (Meta 2024 digital ads) |
| `analysis/data/raw/digital/2026/sample/Ads2026_Digital_Sample_021226.csv` | 25,392 | 27 | Raw input (2026 digital sample) |
| `analysis/data/raw/tv/2024/issues_by_creative/Ads2024_IssuesbyCreative_090124-110624_HSE_AI_013026.csv` | 942,763 | 66 | Raw input (2024 TV issue-coded ads) |
| `analysis/data/raw/tv/2026/sample_with_issues/Ads2026_withissues_Sample_021226.csv` | 61,696 | 150 | Raw input (2026 TV sample with expanded issue flags) |
| `analysis/outputs/week1/harmonized_sample_week1.csv.gz` | 29,830 | 17 | Week 1 derived canonical ad-level text table |
| `analysis/outputs/week1/entity_mentions_week1.csv.gz` | 127,464 | 12 | Week 1 derived NER mention table |
| `analysis/outputs/week1/entity_alias_candidates_week1.csv` | 2,000 | 6 | Week 1 derived alias review queue |
| `analysis/outputs/week1/entity_alias_map_v1.pre_manual_review.csv` | 500 | 9 | Week 2 prep (before manual alias review) |
| `analysis/outputs/week1/entity_alias_map_v1.csv` | 500 | 9 | Week 2 input (after manual alias review) |
| `analysis/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz` | 127,464 | 22 | Week 2 derived mention-level target inference table |
| `analysis/outputs/week2/attack_target_edges_v1.csv` | 5,939 | 10 | Week 2 derived sponsor->target edge list |
| `analysis/outputs/week2/attack_target_nodes_v1.csv` | 9,744 | 9 | Week 2 derived target node table |
| `analysis/scripts/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz` | 127,464 | 22 | Duplicate Week 2 mentions artifact |
| `analysis/scripts/outputs/week2/attack_target_edges_v1.csv` | 5,939 | 10 | Duplicate Week 2 edge artifact |
| `analysis/scripts/outputs/week2/attack_target_nodes_v1.csv` | 9,744 | 9 | Duplicate Week 2 node artifact |

Notes:
- `analysis/scripts/outputs/week2/attack_target_edges_v1.csv` and `analysis/outputs/week2/attack_target_edges_v1.csv` are byte-identical.
- `analysis/scripts/outputs/week2/attack_target_nodes_v1.csv` and `analysis/outputs/week2/attack_target_nodes_v1.csv` are byte-identical.
- The two `entity_mentions_week2_labeled_v1.csv.gz` files differ as compressed binaries, but decompressed content is identical.

## 2) Pipeline lineage (how files were created)

### Raw files (`analysis/data/raw/...`)
These are source/provider exports placed in the repo data tree. They are not generated by local scripts in this project.

### Week 1 derived outputs (`analysis/outputs/week1/...`)
Created in `analysis/notebooks/week1_attack_target_prototype.ipynb`.

Main steps in that notebook:
1. Read raw Google/Meta/TV files in chunks.
2. Platform-specific harmonization (`build_google`, `build_meta`, `build_tv`) to a shared schema.
3. Text cleaning + standardization (`clean_text`, party/tone mapping).
4. Deterministic hash sampling (`sample_frac=0.02`, `max_rows_per_platform=35_000`).
5. NER extraction (spaCy model, currently `en_core_web_sm`) for labels `PERSON`, `ORG`, `GPE`.
6. Create alias candidate queue from mention frequencies.

### Week 2 derived outputs (`analysis/outputs/week2/...`)
Created by:
- `analysis/notebooks/week2_attack_target_pipeline_scaffold.ipynb`, and/or
- `analysis/scripts/week2_build_attack_target_v1.py`

Main steps:
1. Load Week 1 mentions (`entity_mentions_week1.parquet` or `entity_mentions_week1.csv.gz`).
2. Load alias map (`entity_alias_map_v1.csv`) and normalize text for match.
3. Apply canonical aliases to mentions.
4. Compute target signals: `negative_tone`, attack-term context, and self-mention exclusion.
5. Assign `target_confidence` (`high`, `medium`, `low`) and `is_target`.
6. Aggregate to sponsor->target edges and entity-level nodes.

## 3) Raw CSV details

## `analysis/data/raw/digital/2024/google/google2024_set1_20250715.csv.gz`
Purpose:
- Raw Google political ads table for 2024 digital ads.

How created:
- External provider export (source input), then stored compressed as `.csv.gz`.

Entries (schema):
- 92 columns.
- Header includes IDs (`ad_id`, `advertiser_id`), sponsor fields (`advertiser_name`), text (`asr_text`, `ocr_text`), serving windows, spend ranges, targeting, issue model outputs (`ISSUE*`), race/office derivations, federal/entity fields.
- First columns: `ad_id`, `advertiser_name`, `advertiser_id`, `asr_text`, `ocr_text`, `ad_url`, `date_range_start`, `date_range_end`, `num_of_days`, `impressions`, ...

Example entry:
- `advertiser_name=TIM SHEEHY FOR MONTANA`, `date_range_start=2024-08-09`, `date_range_end=2024-09-03`.

## `analysis/data/raw/digital/2024/meta/meta2024_set1_20250714.csv.gz`
Purpose:
- Raw Meta political ads table for 2024 digital ads.

How created:
- External provider export (source input), then stored compressed as `.csv.gz`.

Entries (schema):
- 109 columns.
- Includes Meta IDs and page fields (`ad_id`, `fbid`, `page_id`, `page_name`), text fields, impression/spend ranges, region distributions, model outputs (`goal_*`, `ISSUE*`), party/office/race derivations.
- First columns: `ad_id`, `fbid`, `page_id`, `page_name`, `funding_entity`, `pd_id`, `ad_creative_body`, ...

Example entry:
- `page_name=Ron Odenthal`, `funding_entity=Odenthal4MO`.

## `analysis/data/raw/digital/2026/sample/Ads2026_Digital_Sample_021226.csv`
Purpose:
- Sample 2026 digital ads file for exploratory work.

How created:
- External sample export; stored uncompressed CSV.

Entries (schema):
- 27 columns.
- Includes advertiser/election metadata and campaign calendar fields (`primary`, `special`, `general`).
- Header: `advertiser_type`, `advertiser_party`, `advertiser`, `title`, `advertiser_id`, `state`, `election`, `link`, `market`, `media_type`, `race`, `station`, `language`, `uuid`, `spent`, ...

Example entry:
- `advertiser=Bucks County Republican Committee`, `state=PA`, `election=Bucks County District Attorney 2025 General`.

## `analysis/data/raw/tv/2024/issues_by_creative/Ads2024_IssuesbyCreative_090124-110624_HSE_AI_013026.csv`
Purpose:
- Main 2024 TV file used in Week 1 harmonization.

How created:
- External provider export (issue-by-creative format).

Entries (schema):
- 66 columns.
- Contains airing metadata, spend/GRPs/viewer metrics, issue/tone fields, race/office fields, transcript text.
- Header starts: `advertiser_party`, `advertiser_type`, `advertiser`, `airdate`, `election_state`, `election`, `issue_1`, `issue_2`, `issue_3`, ...

Example entry:
- `advertiser=Titus for NV CD-01`, `airdate=2024-09-13`, `issue_1=Abortion`, `issue_2=Crime`.

## `analysis/data/raw/tv/2026/sample_with_issues/Ads2026_withissues_Sample_021226.csv`
Purpose:
- 2026 TV sample with a broad set of engineered issue indicator columns.

How created:
- External sample export with extended issue coding.

Entries (schema):
- 150 columns.
- Includes core TV fields plus many binary issue flags (`issue_*`), transcript, and scrape marker.

Example entry:
- `advertiser=Williams for Orleans Parish Tax Assessor`, `election=Orleans Parish Tax Assessor 2025 Primary`.

## 4) Week 1 output CSV details

## `analysis/outputs/week1/harmonized_sample_week1.csv.gz`
Purpose:
- Canonical ad-level table aligned across Google/Meta/TV for downstream NLP.

How created:
- In `week1_attack_target_prototype.ipynb` by platform builders + cleaning + deterministic sampling.

Entries (columns):
- `platform`: source platform (`google`, `meta`, `tv`).
- `ad_id`: platform ad identifier (Google `ad_id`, Meta `ad_id`, TV `uuid`).
- `sponsor_name`: normalized sponsor/page/advertiser name field.
- `party_raw`: unstandardized party value from source.
- `party_source`: source column used for party.
- `office_raw`: unstandardized office/category field.
- `tone_raw`: source tone field.
- `tone_source`: where tone came from.
- `issue_context`: compact issue text context.
- `text_main`: cleaned modeling text.
- `date`: source date field.
- `spend_proxy`: numeric spend proxy (or midpoint from lower/upper).
- `text_len`: character count of `text_main`.
- `party_std`: standardized party bucket.
- `party_confidence`: confidence label for party mapping.
- `tone_std`: standardized tone (`NEGATIVE`, `POSITIVE`, `CONTRAST`, `UNKNOWN`, ...).
- `office_std`: standardized office/category bucket.

Key entry distributions:
- Platform rows: `tv=18,652`, `meta=9,579`, `google=1,599`.
- Tone counts: `UNKNOWN=9,538`, `NEGATIVE=8,510`, `POSITIVE=6,089`, `CONTRAST=5,693`.

## `analysis/outputs/week1/entity_mentions_week1.csv.gz`
Purpose:
- Mention-level NER output from harmonized ad text.

How created:
- `extract_entities(...)` in Week 1 notebook using spaCy NER over `text_main`.

Entries (columns):
- `platform`, `ad_id`, `sponsor_name`, `party_std`, `office_std`, `tone_std`, `date`: ad-level context copied to mention rows.
- `entity_text`: extracted entity surface form.
- `entity_label`: NER class (`PERSON`, `ORG`, `GPE`).
- `start_char`, `end_char`: character offsets in document text.
- `context_window`: local text window around the entity.

Key entry distributions:
- Label counts: `PERSON=70,270`, `ORG=30,652`, `GPE=26,542`.
- Platform counts: `tv=97,138`, `meta=24,512`, `google=5,814`.

## `analysis/outputs/week1/entity_alias_candidates_week1.csv`
Purpose:
- Manual-review queue of high-frequency entities to canonicalize aliases.

How created:
- Group Week 1 mentions by `entity_text` + `entity_label`, sort by frequency, normalize candidate canonical form, keep top 2,000.

Entries (columns):
- `entity_text`: raw extracted mention.
- `entity_label`: mention class.
- `mention_count`: frequency in Week 1 mentions.
- `canonical_suggested`: auto-normalized suggestion.
- `needs_review`: review flag.
- `review_notes`: free-text notes for manual adjudication.

Key entry distributions:
- `needs_review=True` for all 2,000 rows.

## `analysis/outputs/week1/entity_alias_map_v1.pre_manual_review.csv`
Purpose:
- Pre-review snapshot of alias map before manual adjudication.

How created:
- Derived from top alias candidates; initialized with default review values.

Entries (columns):
- `entity_text`, `entity_label`, `mention_count`, `canonical_suggested`.
- `canonical_final`: initial canonical value placeholder.
- `review_status`: pre-review status (`PENDING` in this file).
- `action`: proposed action (`KEEP_OR_MERGE`, `DROP`).
- `confidence`: provisional confidence.
- `review_notes`: reviewer comments.

Key entry distributions:
- `review_status=PENDING` for all 500 rows.

## `analysis/outputs/week1/entity_alias_map_v1.csv`
Purpose:
- Reviewed alias map used in Week 2 canonicalization.

How created:
- Manual review/edit pass of pre-review map.

Entries (columns):
- Same schema as pre-review file.
- `canonical_final` and `review_status` reflect adjudicated values.

Key entry distributions:
- `review_status`: `LOCKED=406`, `NEEDS_CONTEXT=94`.
- `action`: `KEEP_OR_MERGE=475`, `DROP=25`.
- `confidence`: `medium=257`, `high=149`, `low=94`.

## 5) Week 2 output CSV details

## `analysis/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz`
Purpose:
- Mention-level table after alias application and target-inference heuristics.

How created:
- `week2_build_attack_target_v1.py`:
  - `apply_aliases(...)`
  - `mark_target_signals(...)`
  - then export with gzip compression.

Entries (columns):
- Original Week 1 mention context: `platform`, `ad_id`, `sponsor_name`, `party_std`, `office_std`, `tone_std`, `date`, `entity_text`, `entity_label`, `start_char`, `end_char`, `context_window`.
- Alias fields: `entity_text_norm`, `canonical_final`, `review_status`, `canonical_entity`, `alias_review_status`.
- Target-signal fields: `negative_tone`, `context_has_attack_term`, `not_self_mention`.
- Target output fields: `target_confidence` (`high`/`medium`/`low`), `is_target` (boolean).

Key entry distributions:
- `target_confidence`: `medium=60,982`, `low=50,128`, `high=16,354`.
- `is_target`: `True=77,336`, `False=50,128`.
- `alias_review_status`: `LOCKED=82,198`, `UNMAPPED=36,486`, `NEEDS_CONTEXT=8,780`.

## `analysis/outputs/week2/attack_target_edges_v1.csv`
Purpose:
- Directed sponsor->target edge list for attack-target network analysis.

How created:
- `build_edges(...)` on rows where `is_target=True`.
- Grouped by `sponsor_name` and `canonical_entity`.

Entries (columns):
- `sponsor_name`: source actor (sponsor).
- `canonical_entity`: inferred target entity.
- `mention_count`: target mention rows on edge.
- `ad_count`: distinct ads contributing to edge.
- `high_confidence_mentions`, `medium_confidence_mentions`: confidence breakdown.
- `platform_count`: number of unique platforms represented.
- `party_mode`: modal sponsor party bucket.
- `tone_mode`: modal tone bucket.
- `edge_confidence`: rule-based confidence (`high` or `medium`).

Key entry distributions:
- `edge_confidence`: `medium=5,370`, `high=569`.
- `party_mode`: `OTHER=2,171`, `DEM=1,909`, `REP=1,763`, `IND=96`.

## `analysis/outputs/week2/attack_target_nodes_v1.csv`
Purpose:
- Entity-level node summary for network analytics and visualization.

How created:
- `build_nodes(...)` grouped by `canonical_entity` across all labeled mentions.

Entries (columns):
- `canonical_entity`: node id/label.
- `mention_count`: total mention rows.
- `ad_count`: distinct ads mentioning entity.
- `sponsor_count`: distinct sponsors mentioning entity.
- `platform_count`: distinct platforms mentioning entity.
- `label_mode`: modal NER type.
- `high_confidence_mentions`, `medium_confidence_mentions`, `low_confidence_mentions`: confidence profile.

Key entry distributions:
- `label_mode`: `PERSON=4,921`, `ORG=3,777`, `GPE=1,046`.

## 6) Duplicate Week 2 CSVs under `analysis/scripts/outputs/week2/`

Files:
- `analysis/scripts/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz`
- `analysis/scripts/outputs/week2/attack_target_edges_v1.csv`
- `analysis/scripts/outputs/week2/attack_target_nodes_v1.csv`

Purpose:
- Extra output location from earlier/alternate script execution context.

Creation:
- Produced by the same Week 2 pipeline logic.

Entry compatibility with canonical outputs:
- `attack_target_edges_v1.csv`: identical to `analysis/outputs/week2/attack_target_edges_v1.csv`.
- `attack_target_nodes_v1.csv`: identical to `analysis/outputs/week2/attack_target_nodes_v1.csv`.
- `entity_mentions_week2_labeled_v1.csv.gz`: decompressed CSV content identical to `analysis/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz`.

## 7) Practical usage guidance

Use these as authoritative by stage:
- Stage 0 (inputs): `analysis/data/raw/...`
- Stage 1 (text/NER prep): `analysis/outputs/week1/harmonized_sample_week1.csv.gz`, `analysis/outputs/week1/entity_mentions_week1.csv.gz`
- Stage 1.5 (manual canonicalization): `analysis/outputs/week1/entity_alias_map_v1.csv`
- Stage 2 (network build): `analysis/outputs/week2/entity_mentions_week2_labeled_v1.csv.gz`, `analysis/outputs/week2/attack_target_edges_v1.csv`, `analysis/outputs/week2/attack_target_nodes_v1.csv`

Unless you need historical reproducibility checks, prefer `analysis/outputs/week2/...` over `analysis/scripts/outputs/week2/...`.
